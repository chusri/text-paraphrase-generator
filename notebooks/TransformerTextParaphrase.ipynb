{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement\n",
    "install allennlp - `pip install allennlp` or https://github.com/allenai/allennlp\n",
    "\n",
    "install tqdm - `pip install tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T00:57:01.191006Z",
     "start_time": "2019-04-29T00:56:59.748818Z"
    }
   },
   "outputs": [],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors import Predictor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model word base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T01:28:51.913110Z",
     "start_time": "2019-04-29T01:28:51.447690Z"
    }
   },
   "outputs": [],
   "source": [
    "archive = load_archive('./load/model1/model.tar.gz',cuda_device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T01:28:53.979479Z",
     "start_time": "2019-04-29T01:28:53.972836Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor = Predictor.from_archive(archive,\"seq2seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T01:28:54.932893Z",
     "start_time": "2019-04-29T01:28:54.889932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sure', '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(\"Are you sure ?\")[\"predicted_tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model character base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T01:28:51.913110Z",
     "start_time": "2019-04-29T01:28:51.447690Z"
    }
   },
   "outputs": [],
   "source": [
    "archive2 = load_archive('./load/model2/model.tar.gz',cuda_device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T01:28:53.979479Z",
     "start_time": "2019-04-29T01:28:53.972836Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor2 = Predictor.from_archive(archive2,\"seq2seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T01:28:54.932893Z",
     "start_time": "2019-04-29T01:28:54.889932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Y', 'o', 'u', ' ', 's', 'u', 'r', 'e', ' ', '?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor2.predict(\"Are you sure ?\")[\"predicted_tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict(sentence):\n",
    "    print(\"Original sentence : {}\".format(sentence))\n",
    "    print(\"Word base model   : {}\".format(\" \".join(predictor.predict(sentence)[\"predicted_tokens\"])))\n",
    "    print(\"Char base model   : {}\".format(\"\".join(predictor2.predict(sentence)[\"predicted_tokens\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"Show me the money!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence : Show me the money!\n",
      "Word base model   : Show me the money .\n",
      "Char base model   : Show me the money .\n"
     ]
    }
   ],
   "source": [
    "_predict(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./load/en-test.txt\",\"r\") as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for line in data:\n",
    "    temp=line.split(\"\\t\")\n",
    "    test.append((temp[1],temp[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "splitter = SpacyWordSplitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate word model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1445/1445 [01:23<00:00, 17.32it/s]\n"
     ]
    }
   ],
   "source": [
    "bleu_1=[]\n",
    "for line in tqdm(test):\n",
    "    bleu_1.append(bleu(\n",
    "        [[i.text for i in splitter.split_words(line[1])]],\n",
    "        predictor.predict(line[0])[\"predicted_tokens\"],\n",
    "        smoothing_function=smoothie\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max : 100.00\n",
      "Min :   0.00\n",
      "Sum :  19.03\n"
     ]
    }
   ],
   "source": [
    "print(\"Max : {:>6.2f}\".format(max(bleu_1)*100))\n",
    "print(\"Min : {:>6.2f}\".format(min(bleu_1)*100))\n",
    "print(\"Sum : {:>6.2f}\".format(sum(bleu_1)/len(test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1445/1445 [01:05<00:00, 22.16it/s]\n"
     ]
    }
   ],
   "source": [
    "bleu_2=[]\n",
    "for line in tqdm(test):\n",
    "    bleu_2.append(bleu(\n",
    "        [[i.text for i in splitter.split_words(line[1])]],\n",
    "        \"\".join(predictor2.predict(line[0])[\"predicted_tokens\"]).split(\" \"),\n",
    "        smoothing_function=smoothie\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max : 100.00\n",
      "Min :   0.00\n",
      "Sum :  17.05\n"
     ]
    }
   ],
   "source": [
    "print(\"Max : {:>6.2f}\".format(max(bleu_2)*100))\n",
    "print(\"Min : {:>6.2f}\".format(min(bleu_2)*100))\n",
    "print(\"Sum : {:>6.2f}\".format(sum(bleu_2)/len(test)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
