{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 21 06:22:09 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   35C    P8     7W / 198W |      2MiB /  8119MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.utils import get_train, convert\n",
    "from utils.preprocess import preprocess_sentence, max_length, tokenize\n",
    "from models.encoder import Encoder\n",
    "from models.decoder import Decoder\n",
    "from models.bahdanauattention import BahdanauAttention\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-alpha0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 40s, sys: 2.42 s, total: 1min 42s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_examples = 1000000\n",
    "num_examples = 500000\n",
    "df = get_train()\n",
    "df_train = df[:num_examples].copy()\n",
    "df_train['firstSentence'] = np.vectorize(preprocess_sentence)(df_train['firstSentence'])\n",
    "df_train['secondSentence'] = np.vectorize(preprocess_sentence)(df_train['secondSentence'])\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 s, sys: 52 ms, total: 14.1 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_tensor, inp_tokenizer = tokenize(df_train['firstSentence'].values.tolist())\n",
    "target_tensor, targ_tokenizer = tokenize(df_train['secondSentence'].values.tolist())\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t<start>\n",
      "7\ti\n",
      "26\tdon\n",
      "4\t'\n",
      "14\tt\n",
      "137\tbelieve\n",
      "44\tin\n",
      "13\tthat\n",
      "3\t.\n",
      "2\t<end>\n",
      "1\t<start>\n",
      "7\ti\n",
      "359\tknew\n",
      "5\tyou\n",
      "88\twould\n",
      "3\t.\n",
      "2\t<end>\n"
     ]
    }
   ],
   "source": [
    "convert(inp_tokenizer, input_tensor_train[5])\n",
    "convert(targ_tokenizer, target_tensor_train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 51]), TensorShape([64, 49]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] *\n",
    "                                   BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden,\n",
    "                                                 enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 51, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 51, 1)\n",
      "Decoder output shape: (batch_size, vocab size) (64, 20101)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.3333\n",
      "Epoch 1 Batch 100 Loss 0.5399\n",
      "Epoch 1 Batch 200 Loss 0.5789\n",
      "Epoch 1 Batch 300 Loss 0.4672\n",
      "Epoch 1 Batch 400 Loss 0.4240\n",
      "Epoch 1 Batch 500 Loss 0.3996\n",
      "Epoch 1 Batch 600 Loss 0.3728\n",
      "Epoch 1 Batch 700 Loss 0.3415\n",
      "Epoch 1 Batch 800 Loss 0.3694\n",
      "Epoch 1 Batch 900 Loss 0.3326\n",
      "Epoch 1 Batch 1000 Loss 0.3275\n",
      "Epoch 1 Batch 1100 Loss 0.3062\n",
      "Epoch 1 Batch 1200 Loss 0.3451\n",
      "Epoch 1 Batch 1300 Loss 0.2661\n",
      "Epoch 1 Batch 1400 Loss 0.2446\n",
      "Epoch 1 Batch 1500 Loss 0.2659\n",
      "Epoch 1 Batch 1600 Loss 0.2793\n",
      "Epoch 1 Batch 1700 Loss 0.1989\n",
      "Epoch 1 Batch 1800 Loss 0.2424\n",
      "Epoch 1 Batch 1900 Loss 0.2191\n",
      "Epoch 1 Batch 2000 Loss 0.1959\n",
      "Epoch 1 Batch 2100 Loss 0.2263\n",
      "Epoch 1 Batch 2200 Loss 0.2051\n",
      "Epoch 1 Batch 2300 Loss 0.1979\n",
      "Epoch 1 Batch 2400 Loss 0.1890\n",
      "Epoch 1 Batch 2500 Loss 0.2489\n",
      "Epoch 1 Batch 2600 Loss 0.2277\n",
      "Epoch 1 Batch 2700 Loss 0.2684\n",
      "Epoch 1 Batch 2800 Loss 0.1979\n",
      "Epoch 1 Batch 2900 Loss 0.1755\n",
      "Epoch 1 Batch 3000 Loss 0.1928\n",
      "Epoch 1 Batch 3100 Loss 0.2159\n",
      "Epoch 1 Batch 3200 Loss 0.2155\n",
      "Epoch 1 Batch 3300 Loss 0.2629\n",
      "Epoch 1 Batch 3400 Loss 0.2193\n",
      "Epoch 1 Batch 3500 Loss 0.2206\n",
      "Epoch 1 Batch 3600 Loss 0.1606\n",
      "Epoch 1 Batch 3700 Loss 0.1911\n",
      "Epoch 1 Batch 3800 Loss 0.2368\n",
      "Epoch 1 Batch 3900 Loss 0.1929\n",
      "Epoch 1 Batch 4000 Loss 0.1654\n",
      "Epoch 1 Batch 4100 Loss 0.1751\n",
      "Epoch 1 Batch 4200 Loss 0.1728\n",
      "Epoch 1 Batch 4300 Loss 0.2043\n",
      "Epoch 1 Batch 4400 Loss 0.1840\n",
      "Epoch 1 Batch 4500 Loss 0.1945\n",
      "Epoch 1 Batch 4600 Loss 0.1765\n",
      "Epoch 1 Batch 4700 Loss 0.1596\n",
      "Epoch 1 Batch 4800 Loss 0.1484\n",
      "Epoch 1 Batch 4900 Loss 0.1603\n",
      "Epoch 1 Batch 5000 Loss 0.1841\n",
      "Epoch 1 Batch 5100 Loss 0.1573\n",
      "Epoch 1 Batch 5200 Loss 0.1922\n",
      "Epoch 1 Batch 5300 Loss 0.1995\n",
      "Epoch 1 Batch 5400 Loss 0.1417\n",
      "Epoch 1 Batch 5500 Loss 0.1818\n",
      "Epoch 1 Batch 5600 Loss 0.1502\n",
      "Epoch 1 Batch 5700 Loss 0.1656\n",
      "Epoch 1 Batch 5800 Loss 0.1519\n",
      "Epoch 1 Batch 5900 Loss 0.1833\n",
      "Epoch 1 Batch 6000 Loss 0.1520\n",
      "Epoch 1 Batch 6100 Loss 0.1718\n",
      "Epoch 1 Batch 6200 Loss 0.1666\n",
      "Epoch 1 Loss 0.2441\n",
      "Time taken for 1 epoch 3689.3627529144287 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.1501\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
